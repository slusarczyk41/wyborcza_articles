06/04/2019 08:44:18 PM INFO: Reading notebook old_articles.ipynb
06/04/2019 08:44:19 PM INFO: Running cell:
from selenium import webdriver
import pandas as pd
from scrap_article_current_tab import scrap_article

from datetime import datetime as dt
from datetime import timedelta as td
from os import makedirs
from os.path import exists
from time import sleep
from json import loads

06/04/2019 08:44:22 PM INFO: Cell returned
06/04/2019 08:44:22 PM INFO: Running cell:
now = dt.now()
year = now.year
month = now.month
day = now.day
ts = str(int(round(now.timestamp())))

06/04/2019 08:44:22 PM INFO: Cell returned
06/04/2019 08:44:22 PM INFO: Running cell:
options = webdriver.ChromeOptions()
options.add_argument('--headless')
chrome = webdriver.Chrome(options=options)

06/04/2019 08:44:28 PM INFO: Cell returned
06/04/2019 08:44:28 PM INFO: Running cell:
# chrome = webdriver.Chrome()

06/04/2019 08:44:28 PM INFO: Cell returned
06/04/2019 08:44:28 PM INFO: Running cell:
chrome.get(url = 'http://wyborcza.pl/0,75248.html?str=100_23719317')

06/04/2019 08:44:34 PM INFO: Cell returned
06/04/2019 08:44:34 PM INFO: Running cell:
sleep(10)

06/04/2019 08:44:44 PM INFO: Cell returned
06/04/2019 08:44:44 PM INFO: Running cell:
# try to click rodo (first click opens sometimes an ad)
for i in range(3):
    try:
        sleep(3)
        chrome.find_element_by_xpath('//*[@id="rodoNotificationWrapper"]/div[1]/div/p[2]')\
            .click()
    except:
        pass

06/04/2019 08:44:54 PM INFO: Cell returned
06/04/2019 08:44:54 PM INFO: Running cell:
# close new window
try:
    chrome.switch_to.window(chrome.window_handles[1])
    chrome.close()
except:
    pass

06/04/2019 08:44:54 PM INFO: Cell returned
06/04/2019 08:44:54 PM INFO: Running cell:
chrome.switch_to.window(chrome.window_handles[0])

06/04/2019 08:44:54 PM INFO: Cell returned
06/04/2019 08:44:54 PM INFO: Running cell:
chrome.get('https://login.wyborcza.pl/')

06/04/2019 08:44:56 PM INFO: Cell returned
06/04/2019 08:44:56 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('//*[@id="wyborczaEmail"]')\
    .send_keys('slusarczyk1@wp.pl')

06/04/2019 08:44:56 PM INFO: Cell returned
06/04/2019 08:44:56 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('//*[@id="wyborczaPassword"]')\
    .send_keys('Sraniejebanko1')

06/04/2019 08:44:56 PM INFO: Cell returned
06/04/2019 08:44:56 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('/html/body/section/section[1]/form/div[4]/button')\
    .click()

06/04/2019 08:44:56 PM INFO: Cell returned
06/04/2019 08:44:56 PM INFO: Running cell:
sleep(5)

06/04/2019 08:45:01 PM INFO: Cell returned
06/04/2019 08:45:01 PM INFO: Running cell:
urls = []
for i in range(100, 1000):
    chrome.get(url = 'http://wyborcza.pl/0,75248.html?str='+str(i)+'_23719317')
    for entry in chrome\
        .find_element_by_xpath('//*[@id="pagetype_index"]/section[5]/div/main/div/div[2]/div/div[2]/ul')\
        .find_elements_by_class_name('entry'):
        
        url = entry.find_element_by_tag_name('h3')\
                .find_element_by_tag_name('a')\
                .get_attribute('href')
        #scrap_article(chrome, url, first_time)
        urls.append(url)
    break
    

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
len(urls)

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
import pandas as pd

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
# df = pd.DataFrame(urls, columns = ['urls'])

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
# df.to_csv('old_urls.csv')

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
df = pd.read_csv('old_urls.csv')

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
errors = []

06/04/2019 08:45:09 PM INFO: Cell returned
06/04/2019 08:45:09 PM INFO: Running cell:
for url in df['urls'].values:
    print(url)
    try:
        a = scrap_article(chrome, url, True)
        with open('data/comments/'+a['url'].split('/')[-1].split(',')[2], 'w') as f:
            f.write(str({'row': {
               'comments': a,
               'timestamp': dt.timestamp(dt.now())
               }
            }))
    except:
        errors.append(url)
    print(a['url'].split('/')[-1].split(',')[2])

