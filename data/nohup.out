07/07/2019 08:24:21 PM INFO: Reading notebook old_articles.ipynb
07/07/2019 08:24:22 PM INFO: Running cell:
from selenium import webdriver
import pandas as pd
from scrap_article_current_tab import scrap_article

from datetime import datetime as dt
from datetime import timedelta as td
from os import makedirs
from os.path import exists
from time import sleep
from json import loads

07/07/2019 08:24:23 PM INFO: Cell returned
07/07/2019 08:24:23 PM INFO: Running cell:
now = dt.now()
year = now.year
month = now.month
day = now.day
ts = str(int(round(now.timestamp())))

07/07/2019 08:24:23 PM INFO: Cell returned
07/07/2019 08:24:23 PM INFO: Running cell:
options = webdriver.ChromeOptions()
options.add_argument('--headless')
chrome = webdriver.Chrome(options=options)

07/07/2019 08:24:24 PM INFO: Cell returned
07/07/2019 08:24:24 PM INFO: Running cell:
# chrome = webdriver.Chrome()

07/07/2019 08:24:24 PM INFO: Cell returned
07/07/2019 08:24:24 PM INFO: Running cell:
chrome.get(url = 'http://wyborcza.pl/0,75248.html?str=100_23719317')

07/07/2019 08:24:31 PM INFO: Cell returned
07/07/2019 08:24:31 PM INFO: Running cell:
sleep(10)

07/07/2019 08:24:41 PM INFO: Cell returned
07/07/2019 08:24:41 PM INFO: Running cell:
# try to click rodo (first click opens sometimes an ad also)
for i in range(3):
    try:
        sleep(3)
        chrome.find_element_by_xpath('//*[@id="rodoNotificationWrapper"]/div[1]/div/p[2]')\
            .click()
    except:
        pass

07/07/2019 08:24:50 PM INFO: Cell returned
07/07/2019 08:24:50 PM INFO: Running cell:
# close new window if exists
try:
    chrome.switch_to.window(chrome.window_handles[1])
    chrome.close()
except:
    pass

07/07/2019 08:24:50 PM INFO: Cell returned
07/07/2019 08:24:50 PM INFO: Running cell:
chrome.switch_to.window(chrome.window_handles[0])

07/07/2019 08:24:50 PM INFO: Cell returned
07/07/2019 08:24:50 PM INFO: Running cell:
chrome.get('https://login.wyborcza.pl/')

07/07/2019 08:24:52 PM INFO: Cell returned
07/07/2019 08:24:52 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('//*[@id="wyborczaEmail"]')\
    .send_keys('slusarczyk1@wp.pl')

07/07/2019 08:24:52 PM INFO: Cell returned
07/07/2019 08:24:52 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('//*[@id="wyborczaPassword"]')\
    .send_keys('Sraniejebanko1')

07/07/2019 08:24:53 PM INFO: Cell returned
07/07/2019 08:24:53 PM INFO: Running cell:
chrome\
    .find_element_by_xpath('/html/body/section/section[1]/form/div[4]/button')\
    .click()

07/07/2019 08:24:53 PM INFO: Cell returned
07/07/2019 08:24:53 PM INFO: Running cell:
sleep(3)

07/07/2019 08:24:56 PM INFO: Cell returned
07/07/2019 08:24:56 PM INFO: Running cell:
# http://wyborcza.pl/0,93566.html?str=2_23721352 for video

07/07/2019 08:24:56 PM INFO: Cell returned
07/07/2019 08:24:56 PM INFO: Running cell:
urls = []
for i in range(1651, 2000):
    chrome.get(url = 'http://wyborcza.pl/0,75248.html?str='+str(i)+'_23719317')
    for entry in chrome\
        .find_element_by_xpath('//*[@id="pagetype_index"]/section[5]/div/main/div/div[2]/div/div[2]/ul')\
        .find_elements_by_class_name('entry'):
        
        url = entry.find_element_by_tag_name('h3')\
                .find_element_by_tag_name('a')\
                .get_attribute('href')
        #scrap_article(chrome, url, first_time)
        urls.append(url)
    break

07/07/2019 08:25:08 PM INFO: Cell returned
07/07/2019 08:25:08 PM INFO: Running cell:
import pandas as pd

07/07/2019 08:25:08 PM INFO: Cell returned
07/07/2019 08:25:08 PM INFO: Running cell:
# df = pd.DataFrame(urls, columns = ['urls'])

07/07/2019 08:25:08 PM INFO: Cell returned
07/07/2019 08:25:08 PM INFO: Running cell:
# df.to_csv('aaa.csv')

07/07/2019 08:25:08 PM INFO: Cell returned
07/07/2019 08:25:08 PM INFO: Running cell:
df = pd.read_csv('44k_urls.csv')

07/07/2019 08:25:09 PM INFO: Cell returned
07/07/2019 08:25:09 PM INFO: Running cell:
errors = []

07/07/2019 08:25:09 PM INFO: Cell returned
07/07/2019 08:25:09 PM INFO: Running cell:
for i, url in enumerate(df['urls']):
    print(url)
    try:
        a = scrap_article(chrome, url, True)
        with open('comments/'+a['url'].split('/')[-1].split(',')[2], 'w') as f:
            f.write(str({'row': {
               'comments': a,
               'timestamp': dt.timestamp(dt.now())
               }
            }))
        print(a['url'].split('/')[-1].split(',')[2])

    except:
        errors.append(url)
        print(url)
        
    

